## Setting up EKS Cluster 

### Setting up eks file 
- First to make things easier create a folder
    - ```terraform```(seperate files inside)-->```eks,sg_eks,vpc```
- Code for the eks(elastic kubernets service) all these files will be in ```eks``` file
- Filename ```eks.tf```
```
resource "aws_iam_role" "master" {
  name = "ed-eks-master"

  assume_role_policy = <<POLICY
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "eks.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
POLICY
}

resource "aws_iam_role_policy_attachment" "AmazonEKSClusterPolicy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
  role       = aws_iam_role.master.name
}

resource "aws_iam_role_policy_attachment" "AmazonEKSServicePolicy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSServicePolicy"
  role       = aws_iam_role.master.name
}

resource "aws_iam_role_policy_attachment" "AmazonEKSVPCResourceController" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSVPCResourceController"
  role       = aws_iam_role.master.name
}

resource "aws_iam_role" "worker" {
  name = "ed-eks-worker"

  assume_role_policy = <<POLICY
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "ec2.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
POLICY
}

resource "aws_iam_policy" "autoscaler" {
  name   = "ed-eks-autoscaler-policy"
  policy = <<EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Action": [
        "autoscaling:DescribeAutoScalingGroups",
        "autoscaling:DescribeAutoScalingInstances",
        "autoscaling:DescribeTags",
        "autoscaling:DescribeLaunchConfigurations",
        "autoscaling:SetDesiredCapacity",
        "autoscaling:TerminateInstanceInAutoScalingGroup",
        "ec2:DescribeLaunchTemplateVersions"
      ],
      "Effect": "Allow",
      "Resource": "*"
    }
  ]
}
EOF

}

resource "aws_iam_role_policy_attachment" "AmazonEKSWorkerNodePolicy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
  role       = aws_iam_role.worker.name
}

resource "aws_iam_role_policy_attachment" "AmazonEKS_CNI_Policy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
  role       = aws_iam_role.worker.name
}

resource "aws_iam_role_policy_attachment" "AmazonSSMManagedInstanceCore" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
  role       = aws_iam_role.worker.name
}

resource "aws_iam_role_policy_attachment" "AmazonEC2ContainerRegistryReadOnly" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
  role       = aws_iam_role.worker.name
}

resource "aws_iam_role_policy_attachment" "x-ray" {
  policy_arn = "arn:aws:iam::aws:policy/AWSXRayDaemonWriteAccess"
  role       = aws_iam_role.worker.name
}
resource "aws_iam_role_policy_attachment" "s3" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"
  role       = aws_iam_role.worker.name
}

resource "aws_iam_role_policy_attachment" "autoscaler" {
  policy_arn = aws_iam_policy.autoscaler.arn
  role       = aws_iam_role.worker.name
}

resource "aws_iam_instance_profile" "worker" {
  depends_on = [aws_iam_role.worker]
  name       = "ed-eks-worker-new-profile"
  role       = aws_iam_role.worker.name
}

###############################################################################################################
resource "aws_eks_cluster" "eks" {
  name = "devops_udemy_eks01"                    // Update according to your naming convention
  role_arn = aws_iam_role.master.arn

  vpc_config {
    subnet_ids = [var.subnet_ids[0],var.subnet_ids[1]]
  }
  
  depends_on = [
    aws_iam_role_policy_attachment.AmazonEKSClusterPolicy,
    aws_iam_role_policy_attachment.AmazonEKSServicePolicy,
    aws_iam_role_policy_attachment.AmazonEKSVPCResourceController,
    aws_iam_role_policy_attachment.AmazonEKSVPCResourceController,
    #aws_subnet.pub_sub1,
    #aws_subnet.pub_sub2,
  ]

}
#################################################################################################################

resource "aws_eks_node_group" "backend" {
  cluster_name    = aws_eks_cluster.eks.name
  node_group_name = "dev"                           // Can you modify accordingly
  node_role_arn   = aws_iam_role.worker.arn
  subnet_ids = [var.subnet_ids[0],var.subnet_ids[1]]
  capacity_type = "ON_DEMAND"
  disk_size = "20"
  instance_types = ["t2.small"]            
  remote_access {
    ec2_ssh_key = "devops_proj_udemey"          // Have to attach the key used initially to configure terraform with aws
    source_security_group_ids = [var.sg_ids]
  } 
  
  labels =  tomap({env = "dev"})

  // you can modify according to your need
  scaling_config {
    desired_size = 2
    max_size     = 3
    min_size     = 1
  }

  update_config {
    max_unavailable = 1
  }

  depends_on = [
    aws_iam_role_policy_attachment.AmazonEKSWorkerNodePolicy,
    aws_iam_role_policy_attachment.AmazonEKS_CNI_Policy,
    aws_iam_role_policy_attachment.AmazonEC2ContainerRegistryReadOnly,
    #aws_subnet.pub_sub1,
    #aws_subnet.pub_sub2,
  ]
}
```
- Now will have ```output.tf``` this .tf file goes in ```eks``` file
- ```output "endpoint"```: Which allows you to extract information about your infrastructure that we have created adter its been provisioned which makes it accessible outside the terraform configuration.
- ```value = aws_eks_cluster.eks.endpoint```: This is basically when the O/P of the ```endpoint``` and the endpoint of the ```EKS``` cluster which is referred to by ```aws_eks_cluster.eks.endpoint```. where ```aws_eks_cluster``` is the resource block for your EKS and ```endpoint``` is an attribute that returns the URL of the API server endpoint for your EKS.
```
output "endpoint" {
  value = aws_eks_cluster.eks.endpoint
}
```
- Now will have ```variable.tf``` this .tf file goes in ```eks``` file. So, this
- This is where we declare the variables in ```eks.tf```
```
variable "sg_ids" {
type = string
}

variable "subnet_ids" {
  type = list
}

variable "vpc_id" {
   //default = "vpc-5f680722"
   type = string
}
```


### Setting up sg_eks file

- Now we have ```output.tf``` this .tf file goes in ```sg_eks``` file
```
output "security_group_public" {
   value = "${aws_security_group.worker_node_sg.id}"
}
```
- Now we have ```sg.tf``` this .tf file goes in ```sg_eks``` file
```
resource "aws_security_group" "worker_node_sg" {
  name        = "eks-test"
  description = "Allow ssh inbound traffic"
  vpc_id      =  var.vpc_id

  ingress {
    description      = "ssh access to public"
    from_port        = 22
    to_port          = 22
    protocol         = "tcp"
    cidr_blocks      = ["0.0.0.0/0"]
  }

  egress {
    from_port        = 0
    to_port          = 0
    protocol         = "-1"
    cidr_blocks      = ["0.0.0.0/0"]
  }

}
```
- Now we have ```variable.tf``` this .tf file goes in ```sg_eks``` file
```
variable "vpc_id" {
   //default = "vpc-5f680722"
   type = string
}
```
### Setting up vpc file
- This ```vpc.tf``` file is we have it already before when we created the aws infra, we are modifying it according to the eks setup.
    - ```// Creating Public subnet-02```
    - ```// Route Table assosciation 2```
    - ```// calling sg_eks module and eks module ```
- The above are the additional modules attached.
```
// Decalring the Cloud provider

provider "aws" {
  region = "us-east-1"

}

// Creating EC2

resource "aws_instance" "dev_proj_ec2" {
  ami                    = "ami-0a0e5d9c7acc336f1"
  instance_type          = "t2.small"
  key_name               = "devops_proj_udemey"
  vpc_security_group_ids = [aws_security_group.dev_proj_sg.id]
  subnet_id              = aws_subnet.dev_proj_pubsubnet_01.id

  //We use for_each to create 3 systems mentioned below,by using for_each 
  // you can create how many systems you need/ requirement
  for_each = toset(["Jenkins_master", "Jenkins_slave", "Ansible"])
  tags = {
    Name = "${each.key}"
  }

}


// Creating Security Group

resource "aws_security_group" "dev_proj_sg" {
  name   = "dev_proj_sg"
  vpc_id = aws_vpc.dev_proj_vpc.id

  ingress {
    description = "SSH Access"
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }


  // Jenkins default port 
  ingress {
    description = "Jenkins GUI Access"
    from_port   = 8080
    to_port     = 8080
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "dev_proj_sg"
  }
}

// Creating VPC

resource "aws_vpc" "dev_proj_vpc" {
  cidr_block       = "10.0.0.0/16"
  instance_tenancy = "default"

  tags = {
    Name = "dev_proj_vpc"
  }
}

// Creating Public subnet

resource "aws_subnet" "dev_proj_pubsubnet_01" {
  vpc_id                  = aws_vpc.dev_proj_vpc.id
  cidr_block              = "10.0.1.0/24"
  map_public_ip_on_launch = "true"
  availability_zone       = "us-east-1a"

  tags = {
    Name = "dev_proj_pubsubnet_01"
  }
}


// Creating Public subnet-02

resource "aws_subnet" "dev_proj_pubsubnet_02" {
  vpc_id                  = aws_vpc.dev_proj_vpc.id
  cidr_block              = "10.0.2.0/24"
  map_public_ip_on_launch = "true"
  availability_zone       = "us-east-1b"

  tags = {
    Name = "dev_proj_pubsubnet_02"
  }
}


// Creating IGW

resource "aws_internet_gateway" "dev_proj_igw" {
  vpc_id = aws_vpc.dev_proj_vpc.id

  tags = {
    Name = "dev_proj_igw"
  }
}

// Creating Route table

resource "aws_route_table" "dev_proj_rt" {
  vpc_id = aws_vpc.dev_proj_vpc.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.dev_proj_igw.id
  }

  tags = {
    Name = "dev_proj_rt"
  }
}

// Route Table assosciation
resource "aws_route_table_association" "dev_proj_rta_pubsubnet_1" {
  subnet_id      = aws_subnet.dev_proj_pubsubnet_01.id
  route_table_id = aws_route_table.dev_proj_rt.id
}

// Route Table assosciation 2
resource "aws_route_table_association" "dev_proj_rta_pubsubnet_2" {
  subnet_id      = aws_subnet.dev_proj_pubsubnet_02.id
  route_table_id = aws_route_table.dev_proj_rt.id
}


// calling sg_eks module and eks module 
module "sgs" {
  source = "../sg_eks"
  vpc_id = aws_vpc.dev_proj_vpc.id
}

module "eks" {
  source     = "../eks"
  vpc_id     = aws_vpc.dev_proj_vpc.id
  subnet_ids = [aws_subnet.dev_proj_pubsubnet_01.id, aws_subnet.dev_proj_pubsubnet_02.id]
  sg_ids     = module.sgs.security_group_public
}
```
- This ```terraform.tfstate``` make sure you keep this file in the terraform folder and then run the terraform commands after pasting this file in the ```terrafom``` folder directory and then run the commands
```
terraform init
terraform fmt
terraform validate
terraform plan
terraform apply
```
- After executing the above commmands the resources get created
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#### After executing the file when the resource are created if you want to stop them there are two ways
    ## Step1:
    - go to ```eks file```
    - Open ```eks.tf```
    - Under ```resource "aws_eks_node_group" "backend"```
    - Under ```scaling_config ```
    ```
    desired_size = 2
    max_size     = 3
    min_size     = 1

    you can put 0's for all of them
    ```
    - You can run ```terraform apply``` directly instead running all of them
    ## Step 2:
    - Go to ```vpc```` folder
    - Open ```pl-ec2.tf```
    - Go to ```// calling sg_eks module and eks module ``` and comment out the whole module
    - And run ```terrafom apply``` if your doubt about it you can run fmt,validate
- Make sure you dont delete the clusters manually because its in auto-scaling mode so recreats again follow the above step 1 & 2 to delete the clusters
 
    
